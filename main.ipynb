{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math, copy, os, time, enum, argparse\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.optim import Adam\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "from torch.hub import download_url_to_file\r\n",
    "\r\n",
    "from torchtext.data import Dataset, BucketIterator, Field, Example\r\n",
    "from torchtext.data.utils import interleave_keys\r\n",
    "from torchtext import datasets\r\n",
    "from torchtext.data import Example\r\n",
    "import spacy\r\n",
    "\r\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BASELINE_MODEL_NUMBER_OF_LAYERS = 6\r\n",
    "BASELINE_MODEL_DIMENSION = 512\r\n",
    "BASELINE_MODEL_NUMBER_OF_HEADS = 8\r\n",
    "BASELINE_MODEL_DROPOUT_PROB = 0.1\r\n",
    "BASELINE_MODEL_LABEL_SMOOTHING_VALUE = 0.1\r\n",
    "\r\n",
    "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\r\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\r\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\r\n",
    "\r\n",
    "os.makedirs(CHECKPOINTS_PATH, exist_ok = True)\r\n",
    "os.makedirs(BINARIES_PATH, exist_ok = True)\r\n",
    "os.makedirs(DATA_DIR_PATH, exist_ok = True)\r\n",
    "\r\n",
    "BOS_TOKEN = '<s>'\r\n",
    "EOS_TOKEN = '</s>'\r\n",
    "PAD_TOKEN = '<pad'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 1 : Understanding the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Transformer(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, src_vocab_size, tar_vocab_size, n_heads, n_layers, p_dropout, log_att_w = False):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # Embeds source/target token ids into embedding vectors.\r\n",
    "        self.src_embedding = Embedding(src_vocab_size, model_dim)\r\n",
    "        self.tar_embedding = Embedding(trg_vocab_size, model_dim)\r\n",
    "\r\n",
    "        # Positional embedding.\r\n",
    "        self.src_pos_embedding = PositionalEncoding(model_dim, p_dropout)\r\n",
    "        self.tar_pos_embedding = PositionalEncoding(model_dim, p_dropout)\r\n",
    "\r\n",
    "        # Embedding get deep-copied multiple times.\r\n",
    "        mha = MultiHeadAttention(model_dim, n_heads, p_dropout)\r\n",
    "        pwn = PositionwiseFeedForwardNet(moidel_dim, p_dropout)\r\n",
    "        encoder_layer = EncoderLayer(model_dim, p_dropout, mha, pwn)\r\n",
    "        decoder_layer = DecoderLayer(model_dim, p_dropout, mha, pwn)\r\n",
    "\r\n",
    "        # Encoder/Decoder stacks.\r\n",
    "        self.encoder = Encoder(encoder_layer, n_layers)\r\n",
    "        self.decoder = Decoder(decoder_layer, n_layers)\r\n",
    "\r\n",
    "        # To convert target token representations into log probability vectors of tar vocabulary size.\r\n",
    "        # We use log probability vectors because torch's KLDivLoss expects log probabilities.\r\n",
    "        self.decoder_generator = DecoderGenerator(model_dim, tar_vocab_size)\r\n",
    "        self.init_params()\r\n",
    "\r\n",
    "    def init_params(self):\r\n",
    "        for name, p in self.model.parameters():\r\n",
    "            if p_dim() > 1:\r\n",
    "                torch.nn.init.xavier_unifrom(p)\r\n",
    "\r\n",
    "    def forward(self, src_token_ids_batch, tar_token_ids_batch, src_mask, tar_mask):\r\n",
    "        src_repr_batch = self.encode(src_token_ids_batch, src_mask)\r\n",
    "        tar_log_probs = self.decode(tar_token_ids_batch, src_repr_batch, tar_mask, src_mask)\r\n",
    "        return tar_log_probs\r\n",
    "\r\n",
    "    def encode(self, src_token_ids_batch, src_mask):\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        Args:\r\n",
    "            src_token_ids_batch ([B, S, D]): [batch_size, longest_src_token, model_dim] the encoder stack preserves this shape.\r\n",
    "            src_mask ([type]): [description]\r\n",
    "        \"\"\"\r\n",
    "        src_embeddings_batch = self.src_embedding(src_token_ids_batch) # get embedding vectors for src token ids.\r\n",
    "        src_embeddings_batch = self.src_pos_embedding(src_embeddings_batch) # add positional embedding.\r\n",
    "        src_repr_batch = self.encoder(src_embeddings_batch, src_mask) # Forward pass through the encoder.\r\n",
    "        return src_repr_batch\r\n",
    "\r\n",
    "    def decode(self, tar_token_ids_batch, src_repr_batch, tar_mask, src_mask):\r\n",
    "        tar_embeddings_batch = self.tar_embedding(tar_token_ids_batch, src_repr_batch, tar_mask, src_mask) # get embedding vector for tar token ids.\r\n",
    "        tar_embeddings_batch = self.tar_pos_embedding(tar_embeddings_batch) # add positional embedding.\r\n",
    "\r\n",
    "        # shape (b, t, d) batch_size, longest tar token sequence length, model_dim\r\n",
    "        tar_repr_batch = self.decoder(tar_embedding_batch, src_repr_batch, tar_mask, src_mask)\r\n",
    "        # (b, t, v) withg v the vocab size.\r\n",
    "        # decoder generator does linear projection + log softmax.\r\n",
    "        tar_log_probs = self.decoder_generator(tar_representations_batch)\r\n",
    "        # reshape into (b*t, v) to pass in KL-div loss.\r\n",
    "        tar_log_probs = tar_log_probs.reshape(-1, tar_log_probs.shape[-1])\r\n",
    "        return tar_log_probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. tokens get embedded into source sequences.\r\n",
    "2. encoder takes a batch of the source sequences.\r\n",
    "3. encoder mixes sources sequences through 6 layers of the base transformer via attention.\r\n",
    "4. the final output gets consumed by the decoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Encoder(torch.nn.Module):\r\n",
    "    def __init__(self, encoder_layer, n_layers):\r\n",
    "        super().__init__()\r\n",
    "        assert isinstance(encoder_layer, EncoderLayer), f'Expected Encoder layer, got {type(encoder_layer)} instead !'\r\n",
    "\r\n",
    "        # get a list of of the encoder layers.\r\n",
    "        self.encoder_layers = get_clones(encoder_layer, n_layers)\r\n",
    "        self.norm = torch.nn.LayerNorm(encoder_layer.model_dim)\r\n",
    "\r\n",
    "    def forward(self, src_embeddings_batch, src_mask):\r\n",
    "        src_representations_batch = src_embeddings_batch\r\n",
    "        # the role of the source mask is to ignore padded token representations in the multi head self attention module.\r\n",
    "        for encoder_layer in self.encoder_layers:\r\n",
    "            src_representations_batch = encoder_layer(src_representations_batch, src_mask)\r\n",
    "        return self.norm(src_representations_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class EncoderLayer(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout, multihead_att, pointwise_net):\r\n",
    "        super().__init__()\r\n",
    "        n_sublayers_encoder = 2\r\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dim, p_dropout), n_sublayers_encoder)\r\n",
    "\r\n",
    "        self.multihead_att = multihead_att\r\n",
    "        self.pointwise_net = pointwise_net\r\n",
    "\r\n",
    "        self.model_dimension = model_dimension\r\n",
    "    \r\n",
    "    def forward(self, src_repr_batch, src_mask):\r\n",
    "        # define a lambda function that takes src_repr_batch as input to have a uniform inteface for the sublayer logic.\r\n",
    "        encoder_self_attention = lambda srb: self.multihead_att(query = srb, key = srb, value = srb, mask = src_mask)\r\n",
    "        # self-attention mha sublayer followed by pointwise feed forward sublayer.\r\n",
    "        # sublayerLogic takes as input the data and the logic it should execute (attention/feedforward)\r\n",
    "        src_repr_batch = self.sublayers[0](src_repr_batch, encoder_self_attention)\r\n",
    "        src_repr_batch = self.sublayers[1](src_repr_batch, self.pointwise_net)\r\n",
    "\r\n",
    "        return src_repr_batch\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " 1. target sequences with embedded tokens.\r\n",
    " 2. 6 iterations of mixing via attention while attending to source token representations.\r\n",
    " 3. final output sends target token representations into the decoder generator.\r\n",
    " 4. target tokens are converted to log probabilities.\r\n",
    "\r\n",
    "The decoder uses causal masking to prevent tokens from looking into the future.\r\n",
    "\r\n",
    "<img src=\"images/causal_mask.PNG\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Decoder(torch.nn.Module):\r\n",
    "    def __init__(self, decoder_layer, n_layers):\r\n",
    "        super().__init__()\r\n",
    "        assert isinstance(decoder_layer, DecoderLayer), f'Expected DecoderLayer, got {type(decoder_layer)} !'\r\n",
    "\r\n",
    "        self.encoder_layers = get_clones(decoder_layer, n_layers)\r\n",
    "        self.norm = torch.nn.LayerNorm(decoder_layer.model_dimension)\r\n",
    "    \r\n",
    "    def forward(self, tar_embedding_batch, src_repr_batch, tar_mask, src_mask):\r\n",
    "        tar_repr_batch = tar_embedding_batch\r\n",
    "\r\n",
    "        # Forward pass through decoder stack.\r\n",
    "        for decoder_layer in self.decoder_layers:\r\n",
    "            # target mask masks pad tokens + future tokens.\r\n",
    "            tar_repr_batch = decoder_layer(tar_repr_batch, src_repr_batch, tar_mask, src_mask)\r\n",
    "\r\n",
    "        return self.norm(tar_repr_batch)\r\n",
    "    \r\n",
    "class DecoderLayer(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout, multihead_att, pointwise_net):\r\n",
    "        super().__init__()\r\n",
    "        n_sublayers_decoder = 3\r\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dim, p_dropout), n_sublayers_decoder)\r\n",
    "        self.tar_multihead_att = copy.deepcopy(multihead_att)\r\n",
    "        self.src_multihead_att = copy.depcopy(multihead_att)\r\n",
    "        self.pointwise_net = pointwise_net\r\n",
    "        self.model_dimension = model_dim\r\n",
    "    \r\n",
    "    def forward(self, tar_repr_batch, src_repr_batch, tar_mask, src_mask):\r\n",
    "        # the inputs that are not passed into lambdas (masks and source representation batches) are cached.\r\n",
    "        srb = src_repr_batch\r\n",
    "        decoder_tar_self_att = lambda trb: self.tar_multihead_att(query = trb, key = trb, vavlue = trb, mask = tar_mask)\r\n",
    "        decoder_src_att = lambda trb: self.src_multihead_att(query = trb, key = srb, value = srb, mask = src_mask)\r\n",
    "\r\n",
    "        # self-attention multihead attention sublayer followed by a source-attending multihead attention and pointwise feed forward net sublayer.\r\n",
    "        tar_repr_batch = self.sublayers[0](tar_repr_batch, decoder_tar_self_att)\r\n",
    "        tar_repr_batch = self.sublayers[1](tar_repr_batch, decoder_src_att)\r\n",
    "        tar_repr_batch = self.sublayers[2](tar_repr_batch, self.pointwise_net)\r\n",
    "        return tar_repr_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the **decoder generator** :\r\n",
    "1. Projects the final decoder token representation. D --> V\r\n",
    "2. applies log softmax"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecoderGenerator(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, vocab_size):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = torch.nn.Linear(model_dim, vocab_size)\r\n",
    "        # linear layer has shape (B, T, V). B batch_size, T max_target_token_sequences, V_target_vocab_size.\r\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim = -1)\r\n",
    "\r\n",
    "    def forward(self, tar_repr_batch):\r\n",
    "        return self.log_softmax(self.linear(tar_repr_batch))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Positional encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "this is what the positional encoding traditionally looks like : \r\n",
    "<img src=\"images/pos_encoding.jpg\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SublayerLogic(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout):\r\n",
    "        super().__init__()\r\n",
    "        self.norm = torch.nn.LayerNorm(model_dim)\r\n",
    "        self.dropout = torch.nn.Dropout(p = p_dropout)\r\n",
    "        # In the original paper, layer norm is doe after the residual connection but experiments proved to be more effective before.\r\n",
    "    \r\n",
    "    def forward(self, repr_batch, sublayer_module):\r\n",
    "        return repr_batch + self.dropout(sublayer_module(self.norm(repr_batch)))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PositionwiseFeedForwardNet(torch.nn.Module):\r\n",
    "    \"\"\"position-wise because the feed-forward net is independantly applied to every token's representation.\r\n",
    "    i.e same as a nested loop going over the batch size and max token sequence length dimensions then applies the network to the sequence representation.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        torch ([type]): [description]\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, model_dim, p_dropout, width_mult = 4):\r\n",
    "        super().__init__()\r\n",
    "        self.linear1 = torch.nn.Linear(model_dim, width_mult*model_dim)\r\n",
    "        self.linear2 = torch.nn.Linear(width_mult*model_dim, model_dim)\r\n",
    "    \r\n",
    "        # dropout layer not mentionned in the paper but commonly used to avoid overfitting.\r\n",
    "        self.dropout = torch.nn.Dropout(p = p_dropout)\r\n",
    "        self.relu = torch.nn.ReLU()\r\n",
    "\r\n",
    "        # representations batch : (B, S/T, D) (batch_size, max_token_sequence_length, model_dim)\r\n",
    "    \r\n",
    "    def forward(self, repr_batch):\r\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(repr_batch))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Embedding(torch.nn.Module):\r\n",
    "    def __init__(self, vocab_size, model_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.embeddings_table = torch.nn.Embedding(vocab_size, model_dim)\r\n",
    "        self.model_dim = model_dim\r\n",
    "    \r\n",
    "    def forward(self, token_ids_batch):\r\n",
    "        assert token_ids_batch.ndim == 2, f'Expected : (batch_size, max_token_seq_length), got {token_ids_batch.shape}'\r\n",
    "        # token_ids_batch has size (B, S/T)\r\n",
    "        # final sequence wille be (B, S/T, D) witjh the model dimensions, so every token id has an associated vector.\r\n",
    "        embeddings = self.embeddings_table(token_ids_batch)\r\n",
    "        # we multiply the embedding weights by the squre root of the model dimension as stated in the paper.\r\n",
    "        return embeddings*math.sqrt(self.model_dim)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PositionEncoding(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout, expected_max_seq_length = 5000):\r\n",
    "        super().__init__()\r\n",
    "        self.dropout = torch.nn.Dropout(p = p_dropout)\r\n",
    "        # as suggested in the paper, use sine frequencies to form a geometric progression as position encodings.\r\n",
    "        position_id = torch.arange(0, expected_max_seq_length).unsqueeze(1)\r\n",
    "        frequencies = torch.pow(10000., -torch.arange(0, model_dim, 2, dtype = torch.float)/model_dim)\r\n",
    "        \r\n",
    "        positional_encodings_table = torch.zeros(expected_max_seq_length, model_dim)\r\n",
    "        positional_encodings_table[0, 0::2] = torch.sin(position_id*frequencies) # sine on even positions.\r\n",
    "        positional_encodings_table[:, 1::2] = torch.cos(position_id*frequencies) # cosine on odd positions.\r\n",
    "\r\n",
    "        # register buffer in order to save the positional encodings table inside the state dict even though these are not trainable.\r\n",
    "        # So if we don't register them to the buffer, they will not be saved in the state dict.\r\n",
    "        self.register_buffer('positional_encodings_table', positional_encodings_table)\r\n",
    "\r\n",
    "    def forward(self, embeddings_batch):\r\n",
    "        assert embeddings_batch.ndim == 3 and embeddings_batch.shape[-1] == self.positional_encodings_table.shape[1], f'Expected (batch_size, max_token_sequence_length, model_dimension and got {embeddings_batch.shape}'\r\n",
    "        # embeddings_batch.shape (B, S/T, D)\r\n",
    "        # transformed into (S/T, D) that ill be broadcasted to (B, S/T, D) before adding it to the embedding.\r\n",
    "        positional_encodings = self.encodings_table[:embeddings_batch.shape[1]]\r\n",
    "        # then apply dropout to the sum of the positional encodings and token embeddings.\r\n",
    "        return self.dropout(embeddings_batch + positional_encodings)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_clones(module, n_deep_clones):\r\n",
    "    # creates independent modules so that each clones weights can be independantly updated.\r\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(n_deep_copies)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multihead Attention"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiHeadAttention(torch.nn.Modue):\r\n",
    "    def __init__(self, model_dimension, n_heads, p_dropout, log_att_weights):\r\n",
    "        super().__init__()\r\n",
    "        assert model_dim%n_heads == 0, 'model dimension must be divisible by the number of heads !'\r\n",
    "        self.head_dim = int(model_dim/n_heads)\r\n",
    "        self.n_heads = n_heads\r\n",
    "\r\n",
    "        self.qkv_nets = get_clones(torch.nn.Linear(model_dim/n_heads))\r\n",
    "        self.out_projection_net = torch.nn.Linear(model_dim, model_dim)\r\n",
    "        \r\n",
    "        self.attention_dropout = torch.nn.Dropout(p= p_dropout)\r\n",
    "        self.softmax = torch.nn.Softmax(dim = -1)\r\n",
    "\r\n",
    "        self.log_att_weights = log_att_weights\r\n",
    "        self.attention_weights = None\r\n",
    "    \r\n",
    "    def attention(self, query, key, value, mask):\r\n",
    "        # Step 1 : scaled dot product attentin.\r\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.head_dim)\r\n",
    "        # Step 2 : Optionally mask tokens whose representation we want to ignore by setting a big negative number on loations we want to ignore.\r\n",
    "        # mask shape (B, 1 1, S) or (B, A, T, T) will get broadcasted to match the score shape.\r\n",
    "        if mask is not None:\r\n",
    "            scores.masked_fill_(mask == torch.tensor(False), float('-inf'))\r\n",
    "        # Step 3 : Calculate attention weights.\r\n",
    "        att_weights = self.softmax(scores)\r\n",
    "        # Step 4 : apply dropout to attention weights aswell.\r\n",
    "        att_weights = self.attention_dropout(att_weights)\r\n",
    "        # Step 5 : calculate new token representations based on attention weights.\r\n",
    "        # (B NH, S, HD) --> (B, NH, T, HD)\r\n",
    "        intermediate_token_repr = torch.matmul(att_weights, value)\r\n",
    "        return intermediate_token_repr, att_weights\r\n",
    "    \r\n",
    "    def forward(self, query, key, value, mask):\r\n",
    "        batch_size = query.shape[0]\r\n",
    "        # Step 1 : input linear projection\r\n",
    "        query, key, value = [net(x).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) for net, x in zip(self.qkv_nets, (query, key, value))]\r\n",
    "        # Step 2 : apply attention, compare query with key and use it to combine values.\r\n",
    "        intermediate_token_repr, att_weights = self.attention(query, key, value, mask)\r\n",
    "        # log the attention weights for visualization purposes, can be turned off during training to avoid memory problems.\r\n",
    "        # Step 3 : reshape from (B, NH, S/T, HD) over (B, S/T, NH, HD) into (B, S/T, NH*HD) (shape of the input to the forward function).\r\n",
    "        reshaped = intermediate_token_repr.transpose(1, 2).reshape(batch_size, -1, self.n_heads*self.head_dim)\r\n",
    "        # Step 4 : output linear projection.\r\n",
    "        token_repr  = self.out_projection_net(reshaped)\r\n",
    "        return token_repr\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2 : Data pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DatasetType(enum.Enum):\r\n",
    "    IWSLT = 0,\r\n",
    "    WMT14 = 1\r\n",
    "\r\n",
    "class LanguageDirection(enum.Enum):\r\n",
    "    E2G = 0,\r\n",
    "    G2E = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_datasets_and_vocabs(dataset_path, language_direction, use_iwslt = True, use_caching_mechanism = True):\r\n",
    "    german_to_english = language_direction == LanguageDirection.G2E.name\r\n",
    "    spacy_de = spacy.load('de_core_news_sm')\r\n",
    "    spacy_en = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "    def tokenize_de(text):\r\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\r\n",
    "\r\n",
    "    def tokenize_en(text):\r\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
    "    \r\n",
    "    src_tokenizer = tokenize_de if german_to_english else tokenize_en\r\n",
    "    tar_tokenizer = tokenize_en if german_to_english else tokenize_de\r\n",
    "    # batch first then channels.\r\n",
    "    src_field_processor = Field(tokenize = src_tokenizer, pad_token = PAD_TOKEN, batch_first = True)\r\n",
    "    tar_field_processor = Field(tokenize = tar_tokenizer, init_token = BOS_TOKEN, eos_token = EOS_TOKEN, pad_token = PAD_TOKEN, batch_first = True)\r\n",
    "\r\n",
    "    fields = [('src', src_field_processor), ('tar', tar_field_processor)]\r\n",
    "    MAX_LEN = 100 # filter out sequences with more than MAX_LEN tokens.\r\n",
    "\r\n",
    "    filter_pred = lambda x: len(x.src) <= MAX_LEN and len(x.tar) <= MAX_LEN\r\n",
    "\r\n",
    "    # Caching to avoid redoing tokenization.\r\n",
    "    prefix = 'de_en' if german_to_english else 'en_de'\r\n",
    "    prefix += '_iwslt' if use_iwslt else '_wmt14'\r\n",
    "    val_cache_path = os.path.join(dataset_path, f'{prefix}_val_cache.csv')\r\n",
    "    test_cache_path = os.path.join(dataset_path, f'{prefix}_test_cache.csv')\r\n",
    "\r\n",
    "    ts = time.time()\r\n",
    "    if not use_caching_mechanism or not (os.path.exists(train_cache_path) and os.path.exists(val_cache_path)):\r\n",
    "        # the dataset is made of .tar and .src attributes\r\n",
    "        # so basically a table with 2 columns 'src' and 'tar'\r\n",
    "        src_ext = '.de' if german_to_english else '.en'\r\n",
    "        tar_ext = '.en' if german_to_english else '.de'\r\n",
    "        dataset_split_fn = datasets.IWSLT.splits if use_iwslt else datasets.WMT14.splits\r\n",
    "        train_dataset, val_dataset, test_dataset = dataset_split_fn(exts = (src_ext, tar_ext), fields = fields, root = dataset_path, filter_pred = filter_pred)\r\n",
    "\r\n",
    "        save_cache(train_cache_path, train_dataset)\r\n",
    "        save_cache(val_cache_path, val_dataset)\r\n",
    "        save_cache(test_cache_path, test_dataset)\r\n",
    "    else:\r\n",
    "        train_dataset, val_dataset = DatasetWrapper.get_train_and_val_datasets(train_cache_path, val_cache_path, fields, filter_pred = filter_pred)\r\n",
    "    print(f'Data preparation time : {time.time() - ts:3f}s.')\r\n",
    "\r\n",
    "    MIN_FREQ = 2\r\n",
    "    src_field_processor.build_vocab(train_dataset.src, min_freq = MIN_FREQ)\r\n",
    "    tar_field_processor.build_vocab(train_dataset.tar, min_freq = MIN_FREQ)\r\n",
    "\r\n",
    "    return train_dataset, val_dataset, src_field_processor, tar_field_processor\r\n",
    "    \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_datalaoders(dataset_path, language_direction, dataset_name, batch_size, device):\r\n",
    "    train_dataset, val_dataset, src_field_processor, tar_field_processor = get_datasets_and_vocabs(dataset_path, language_direction, dataset_name = DatasetType.IWSLT.name)\r\n",
    "    train_token_ids_loader, val_token_ids_loader = BucketIterator.splits(datasets = (train_dataset, val_dataset), batch_size = batch_size, device = device, sort_within_batch = True, batch_size_fn = batch_size_fn)\r\n",
    "    return train_token_ids_loader, val_token_ids_loader, src_field_processor, tar_field_processor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "global longest_src_sentence, longest_tar_sentence\r\n",
    "def batch_size_fn(new_example, count, sofar):\r\n",
    "    global longest_src_sentence, longest_tar_sentence\r\n",
    "    if count == 1:\r\n",
    "        longest_src_sentence = 0\r\n",
    "        longest_tar_sentence = 0\r\n",
    "    longest_src_sentence = max(longest_src_sentence, len(new_example.src))\r\n",
    "    longest_tar_sentence = max(longest_tar_sentence, len(new_example.tar)+2)\r\n",
    "\r\n",
    "    num_of_tokens_in_src_tensor = count*longest_src_sentence\r\n",
    "    num_of_tokens_in_tar_tensor = count*longest_tar_sentence\r\n",
    "    return max(num_tokens_in_src_tensor, num_tokens_in_tar_tensor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Caching mechanism"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class FastTranslationDataset(Dataset):\r\n",
    "    @staticmethod\r\n",
    "    def sort_key(ex):\r\n",
    "        # interleaves 16bit representation of lengths.\r\n",
    "        # heuristic that helps the BucketIterator sort bigger batches first.\r\n",
    "        return interleave_keys(len(ex.src), len(ex.tar))\r\n",
    "\r\n",
    "    def __init__(self, cache_path, fields, **kwargs):\r\n",
    "        cached_data = [line.split() for line in open(cache_path, encoding = 'utf-8')]\r\n",
    "        cached_data_src = cached_data[0::2] # even lines contain source examples.\r\n",
    "        cached_data_tar = cached_data[1::2] # odd lines contain target examples\r\n",
    "        assert len(cached_data_src) == len(cached_data_tar), 'Source and target data should be of the same length.'\r\n",
    "        examples = []\r\n",
    "        src_dataset_total_number_of_tokens = 0\r\n",
    "        tar_dataset_total_number_of_tokens = 0\r\n",
    "        for src_tokenized_data, tar_tokenized_data in zip(cached_data_src, cached_data_tar):\r\n",
    "            ex = Example()\r\n",
    "            \r\n",
    "            setattr(ex, 'src', src_tokenized_data)\r\n",
    "            setattr(ex, 'tar', tar_tokenized_data)\r\n",
    "\r\n",
    "            examples.append(ex)\r\n",
    "\r\n",
    "            # Update the number of tokens.\r\n",
    "            src_dataset_total_number_of_tokens += len(src_tokenized_data)\r\n",
    "            tar_dataset_total_number_of_tokens += len(tar_tokenized_data)\r\n",
    "\r\n",
    "        # print relevant information about the dataset.\r\n",
    "        filename_parts = os.path.split(cache_path)[1].split('_')\r\n",
    "        src_language, tar_language = ('English', 'German') if filename_parts[0] == 'en' else ('German', 'English')\r\n",
    "        dataset_name = 'IWSLT' if filename_parts[2] == 'iwslt' else 'WMT-14'\r\n",
    "        dataset_type = 'train' if filename_parts[3] == 'train' else 'val'\r\n",
    "        print(f'{dataset_type} dataset ({dataset_name}) has {src_dataset_total_number_of_tokens} tokens in the SOURCE language ({src_language}) corpus.')\r\n",
    "        print(f'{dataset_type} dataset ({dataset_name}) has {tar_dataset_total_number_of_tokens} tokens in the TARGET language ({tar_language}) corpus.')\r\n",
    "\r\n",
    "        # Call the parent class dataset's constructor.\r\n",
    "        super().__init__(examples, fields, **kwargs)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DatasetWrapper(FastTranslationDataset):\r\n",
    "    @classmethod\r\n",
    "    def get_train_and_val_datasets(cls, train_cache_path, val_cache_path, fields, **kwargs):\r\n",
    "        train_dataset = cls(train_cache_path, fields, **kwargs)\r\n",
    "        val_dataset = cls(val_cache_path, fields, **kwargs)\r\n",
    "\r\n",
    "        return train_dataset, val_dataset\r\n",
    "\r\n",
    "def save_cache(cache_path, dataset):\r\n",
    "    with open(cache_path, 'w', encoding='utf-8') as cache_file:\r\n",
    "        for ex in dataset.examples:\r\n",
    "            cache_file.write(' '.join(ex.src) + '\\n')\r\n",
    "            cache_file.write(' '.join(ex.tar) + '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}