{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math, copy, os, time, enum, argparse\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.optim import Adam\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "from torch.hub import download_url_to_file\r\n",
    "\r\n",
    "from torchtext.data import Dataset, BucketIterator, Field, Example\r\n",
    "from torchtext.data.utils import interleave_keys\r\n",
    "from torchtext import datasets\r\n",
    "from torchtext.data import Example\r\n",
    "import spacy\r\n",
    "\r\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BASELINE_MODEL_NUMBER_OF_LAYERS = 6\r\n",
    "BASELINE_MODEL_DIMENSION = 512\r\n",
    "BASELINE_MODEL_NUMBER_OF_HEADS = 8\r\n",
    "BASELINE_MODEL_DROPOUT_PROB = 0.1\r\n",
    "BASELINE_MODEL_LABEL_SMOOTHING_VALUE = 0.1\r\n",
    "\r\n",
    "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\r\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\r\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\r\n",
    "\r\n",
    "os.makedirs(CHECKPOINTS_PATH, exist_ok = True)\r\n",
    "os.makedirs(BINARIES_PATH, exist_ok = True)\r\n",
    "os.makedirs(DATA_DIR_PATH, exist_ok = True)\r\n",
    "\r\n",
    "BOS_TOKEN = '<s>'\r\n",
    "EOS_TOKEN = '</s>'\r\n",
    "PAD_TOKEN = '<pad'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 1 : Understanding the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Transformer(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, src_vocab_size, tar_vocab_size, n_heads, n_layers, p_dropout, log_att_w = False):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # Embeds source/target token ids into embedding vectors.\r\n",
    "        self.src_embedding = Embedding(src_vocab_size, model_dim)\r\n",
    "        self.tar_embedding = Embedding(trg_vocab_size, model_dim)\r\n",
    "\r\n",
    "        # Positional embedding.\r\n",
    "        self.src_pos_embedding = PositionalEncoding(model_dim, p_dropout)\r\n",
    "        self.tar_pos_embedding = PositionalEncoding(model_dim, p_dropout)\r\n",
    "\r\n",
    "        # Embedding get deep-copied multiple times.\r\n",
    "        mha = MultiHeadAttention(model_dim, n_heads, p_dropout)\r\n",
    "        pwn = PositionwiseFeedForwardNet(moidel_dim, p_dropout)\r\n",
    "        encoder_layer = EncoderLayer(model_dim, p_dropout, mha, pwn)\r\n",
    "        decoder_layer = DecoderLayer(model_dim, p_dropout, mha, pwn)\r\n",
    "\r\n",
    "        # Encoder/Decoder stacks.\r\n",
    "        self.encoder = Encoder(encoder_layer, n_layers)\r\n",
    "        self.decoder = Decoder(decoder_layer, n_layers)\r\n",
    "\r\n",
    "        # To convert target token representations into log probability vectors of tar vocabulary size.\r\n",
    "        # We use log probability vectors because torch's KLDivLoss expects log probabilities.\r\n",
    "        self.decoder_generator = DecoderGenerator(model_dim, tar_vocab_size)\r\n",
    "        self.init_params()\r\n",
    "\r\n",
    "    def init_params(self):\r\n",
    "        for name, p in self.model.parameters():\r\n",
    "            if p_dim() > 1:\r\n",
    "                torch.nn.init.xavier_unifrom(p)\r\n",
    "\r\n",
    "    def forward(self, src_token_ids_batch, tar_token_ids_batch, src_mask, tar_mask):\r\n",
    "        src_repr_batch = self.encode(src_token_ids_batch, src_mask)\r\n",
    "        tar_log_probs = self.decode(tar_token_ids_batch, src_repr_batch, tar_mask, src_mask)\r\n",
    "        return tar_log_probs\r\n",
    "\r\n",
    "    def encode(self, src_token_ids_batch, src_mask):\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        Args:\r\n",
    "            src_token_ids_batch ([B, S, D]): [batch_size, longest_src_token, model_dim] the encoder stack preserves this shape.\r\n",
    "            src_mask ([type]): [description]\r\n",
    "        \"\"\"\r\n",
    "        src_embeddings_batch = self.src_embedding(src_token_ids_batch) # get embedding vectors for src token ids.\r\n",
    "        src_embeddings_batch = self.src_pos_embedding(src_embeddings_batch) # add positional embedding.\r\n",
    "        src_repr_batch = self.encoder(src_embeddings_batch, src_mask) # Forward pass through the encoder.\r\n",
    "        return src_repr_batch\r\n",
    "\r\n",
    "    def decode(self, tar_token_ids_batch, src_repr_batch, tar_mask, src_mask):\r\n",
    "        tar_embeddings_batch = self.tar_embedding(tar_token_ids_batch, src_repr_batch, tar_mask, src_mask) # get embedding vector for tar token ids.\r\n",
    "        tar_embeddings_batch = self.tar_pos_embedding(tar_embeddings_batch) # add positional embedding.\r\n",
    "\r\n",
    "        # shape (b, t, d) batch_size, longest tar token sequence length, model_dim\r\n",
    "        tar_repr_batch = self.decoder(tar_embedding_batch, src_repr_batch, tar_mask, src_mask)\r\n",
    "        # (b, t, v) withg v the vocab size.\r\n",
    "        # decoder generator does linear projection + log softmax.\r\n",
    "        tar_log_probs = self.decoder_generator(tar_representations_batch)\r\n",
    "        # reshape into (b*t, v) to pass in KL-div loss.\r\n",
    "        tar_log_probs = tar_log_probs.reshape(-1, tar_log_probs.shape[-1])\r\n",
    "        return tar_log_probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. tokens get embedded into source sequences.\r\n",
    "2. encoder takes a batch of the source sequences.\r\n",
    "3. encoder mixes sources sequences through 6 layers of the base transformer via attention.\r\n",
    "4. the final output gets consumed by the decoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Encoder(torch.nn.Module):\r\n",
    "    def __init__(self, encoder_layer, n_layers):\r\n",
    "        super().__init__()\r\n",
    "        assert isinstance(encoder_layer, EncoderLayer), f'Expected Encoder layer, got {type(encoder_layer)} instead !'\r\n",
    "\r\n",
    "        # get a list of of the encoder layers.\r\n",
    "        self.encoder_layers = get_clones(encoder_layer, n_layers)\r\n",
    "        self.norm = torch.nn.LayerNorm(encoder_layer.model_dim)\r\n",
    "\r\n",
    "    def forward(self, src_embeddings_batch, src_mask):\r\n",
    "        src_representations_batch = src_embeddings_batch\r\n",
    "        # the role of the source mask is to ignore padded token representations in the multi head self attention module.\r\n",
    "        for encoder_layer in self.encoder_layers:\r\n",
    "            src_representations_batch = encoder_layer(src_representations_batch, src_mask)\r\n",
    "        return self.norm(src_representations_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class EncoderLayer(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout, multihead_att, pointwise_net):\r\n",
    "        super().__init__()\r\n",
    "        n_sublayers_encoder = 2\r\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dim, p_dropout), n_sublayers_encoder)\r\n",
    "\r\n",
    "        self.multihead_att = multihead_att\r\n",
    "        self.pointwise_net = pointwise_net\r\n",
    "\r\n",
    "        self.model_dimension = model_dimension\r\n",
    "    \r\n",
    "    def forward(self, src_repr_batch, src_mask):\r\n",
    "        # define a lambda function that takes src_repr_batch as input to have a uniform inteface for the sublayer logic.\r\n",
    "        encoder_self_attention = lambda srb: self.multihead_att(query = srb, key = srb, value = srb, mask = src_mask)\r\n",
    "        # self-attention mha sublayer followed by pointwise feed forward sublayer.\r\n",
    "        # sublayerLogic takes as input the data and the logic it should execute (attention/feedforward)\r\n",
    "        src_repr_batch = self.sublayers[0](src_repr_batch, encoder_self_attention)\r\n",
    "        src_repr_batch = self.sublayers[1](src_repr_batch, self.pointwise_net)\r\n",
    "\r\n",
    "        return src_repr_batch\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " 1. target sequences with embedded tokens.\r\n",
    " 2. 6 iterations of mixing via attention while attending to source token representations.\r\n",
    " 3. final output sends target token representations into the decoder generator.\r\n",
    " 4. target tokens are converted to log probabilities.\r\n",
    "\r\n",
    "The decoder uses causal masking to prevent tokens from looking into the future.\r\n",
    "\r\n",
    "<img src=\"images/causal_mask.PNG\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Decoder(torch.nn.Module):\r\n",
    "    def __init__(self, decoder_layer, n_layers):\r\n",
    "        super().__init__()\r\n",
    "        assert isinstance(decoder_layer, DecoderLayer), f'Expected DecoderLayer, got {type(decoder_layer)} !'\r\n",
    "\r\n",
    "        self.encoder_layers = get_clones(decoder_layer, n_layers)\r\n",
    "        self.norm = torch.nn.LayerNorm(decoder_layer.model_dimension)\r\n",
    "    \r\n",
    "    def forward(self, tar_embedding_batch, src_repr_batch, tar_mask, src_mask):\r\n",
    "        tar_repr_batch = tar_embedding_batch\r\n",
    "\r\n",
    "        # Forward pass through decoder stack.\r\n",
    "        for decoder_layer in self.decoder_layers:\r\n",
    "            # target mask masks pad tokens + future tokens.\r\n",
    "            tar_repr_batch = decoder_layer(tar_repr_batch, src_repr_batch, tar_mask, src_mask)\r\n",
    "\r\n",
    "        return self.norm(tar_repr_batch)\r\n",
    "    \r\n",
    "class DecoderLayer(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout, multihead_att, pointwise_net):\r\n",
    "        super().__init__()\r\n",
    "        n_sublayers_decoder = 3\r\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dim, p_dropout), n_sublayers_decoder)\r\n",
    "        self.tar_multihead_att = copy.deepcopy(multihead_att)\r\n",
    "        self.src_multihead_att = copy.depcopy(multihead_att)\r\n",
    "        self.pointwise_net = pointwise_net\r\n",
    "        self.model_dimension = model_dim\r\n",
    "    \r\n",
    "    def forward(self, tar_repr_batch, src_repr_batch, tar_mask, src_mask):\r\n",
    "        # the inputs that are not passed into lambdas (masks and source representation batches) are cached.\r\n",
    "        srb = src_repr_batch\r\n",
    "        decoder_tar_self_att = lambda trb: self.tar_multihead_att(query = trb, key = trb, vavlue = trb, mask = tar_mask)\r\n",
    "        decoder_src_att = lambda trb: self.src_multihead_att(query = trb, key = srb, value = srb, mask = src_mask)\r\n",
    "\r\n",
    "        # self-attention multihead attention sublayer followed by a source-attending multihead attention and pointwise feed forward net sublayer.\r\n",
    "        tar_repr_batch = self.sublayers[0](tar_repr_batch, decoder_tar_self_att)\r\n",
    "        tar_repr_batch = self.sublayers[1](tar_repr_batch, decoder_src_att)\r\n",
    "        tar_repr_batch = self.sublayers[2](tar_repr_batch, self.pointwise_net)\r\n",
    "        return tar_repr_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the **decoder generator** :\r\n",
    "1. Projects the final decoder token representation. D --> V\r\n",
    "2. applies log softmax"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecoderGenerator(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, vocab_size):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = torch.nn.Linear(model_dim, vocab_size)\r\n",
    "        # linear layer has shape (B, T, V). B batch_size, T max_target_token_sequences, V_target_vocab_size.\r\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim = -1)\r\n",
    "\r\n",
    "    def forward(self, tar_repr_batch):\r\n",
    "        return self.log_softmax(self.linear(tar_repr_batch))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Positional encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "this is what the positional encoding traditionally looks like : \r\n",
    "<img src=\"images/pos_encoding.jpg\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SublayerLogic(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout):\r\n",
    "        super().__init__()\r\n",
    "        self.norm = torch.nn.LayerNorm(model_dim)\r\n",
    "        self.dropout = torch.nn.Dropout(p = p_dropout)\r\n",
    "        # In the original paper, layer norm is doe after the residual connection but experiments proved to be more effective before.\r\n",
    "    \r\n",
    "    def forward(self, repr_batch, sublayer_module):\r\n",
    "        return repr_batch + self.dropout(sublayer_module(self.norm(repr_batch)))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PositionwiseFeedForwardNet(torch.nn.Module):\r\n",
    "    \"\"\"position-wise because the feed-forward net is independantly applied to every token's representation.\r\n",
    "    i.e same as a nested loop going over the batch size and max token sequence length dimensions then applies the network to the sequence representation.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        torch ([type]): [description]\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, model_dim, p_dropout, width_mult = 4):\r\n",
    "        super().__init__()\r\n",
    "        self.linear1 = torch.nn.Linear(model_dim, width_mult*model_dim)\r\n",
    "        self.linear2 = torch.nn.Linear(width_mult*model_dim, model_dim)\r\n",
    "    \r\n",
    "        # dropout layer not mentionned in the paper but commonly used to avoid overfitting.\r\n",
    "        self.dropout = torch.nn.Dropout(p = p_dropout)\r\n",
    "        self.relu = torch.nn.ReLU()\r\n",
    "\r\n",
    "        # representations batch : (B, S/T, D) (batch_size, max_token_sequence_length, model_dim)\r\n",
    "    \r\n",
    "    def forward(self, repr_batch):\r\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(repr_batch))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Embedding(torch.nn.Module):\r\n",
    "    def __init__(self, vocab_size, model_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.embeddings_table = torch.nn.Embedding(vocab_size, model_dim)\r\n",
    "        self.model_dim = model_dim\r\n",
    "    \r\n",
    "    def forward(self, token_ids_batch):\r\n",
    "        assert token_ids_batch.ndim == 2, f'Expected : (batch_size, max_token_seq_length), got {token_ids_batch.shape}'\r\n",
    "        # token_ids_batch has size (B, S/T)\r\n",
    "        # final sequence wille be (B, S/T, D) witjh the model dimensions, so every token id has an associated vector.\r\n",
    "        embeddings = self.embeddings_table(token_ids_batch)\r\n",
    "        # we multiply the embedding weights by the squre root of the model dimension as stated in the paper.\r\n",
    "        return embeddings*math.sqrt(self.model_dim)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PositionEncoding(torch.nn.Module):\r\n",
    "    def __init__(self, model_dim, p_dropout, expected_max_seq_length = 5000):\r\n",
    "        super().__init__()\r\n",
    "        self.dropout = torch.nn.Dropout(p = p_dropout)\r\n",
    "        # as suggested in the paper, use sine frequencies to form a geometric progression as position encodings.\r\n",
    "        position_id = torch.arange(0, expected_max_seq_length).unsqueeze(1)\r\n",
    "        frequencies = torch.pow(10000., -torch.arange(0, model_dim, 2, dtype = torch.float)/model_dim)\r\n",
    "        \r\n",
    "        positional_encodings_table = torch.zeros(expected_max_seq_length, model_dim)\r\n",
    "        positional_encodings_table[0, 0::2] = torch.sin(position_id*frequencies) # sine on even positions.\r\n",
    "        positional_encodings_table[:, 1::2] = torch.cos(position_id*frequencies) # cosine on odd positions.\r\n",
    "\r\n",
    "        # register buffer in order to save the positional encodings table inside the state dict even though these are not trainable.\r\n",
    "        # So if we don't register them to the buffer, they will not be saved in the state dict.\r\n",
    "        self.register_buffer('positional_encodings_table', positional_encodings_table)\r\n",
    "\r\n",
    "    def forward(self, embeddings_batch):\r\n",
    "        assert embeddings_batch.ndim == 3 and embeddings_batch.shape[-1] == self.positional_encodings_table.shape[1], f'Expected (batch_size, max_token_sequence_length, model_dimension and got {embeddings_batch.shape}'\r\n",
    "        # embeddings_batch.shape (B, S/T, D)\r\n",
    "        # transformed into (S/T, D) that ill be broadcasted to (B, S/T, D) before adding it to the embedding.\r\n",
    "        positional_encodings = self.encodings_table[:embeddings_batch.shape[1]]\r\n",
    "        # then apply dropout to the sum of the positional encodings and token embeddings.\r\n",
    "        return self.dropout(embeddings_batch + positional_encodings)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_clones(module, n_deep_clones):\r\n",
    "    # creates independent modules so that each clones weights can be independantly updated.\r\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(n_deep_copies)])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}